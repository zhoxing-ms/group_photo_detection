{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cd58c-fa9e-4a6b-9955-9bac98d93451",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n",
      "  0%|\u001b[34m          \u001b[0m| 0/1 [00:43<?, ?it/s]\n",
      "  0%|\u001b[34m          \u001b[0m| 0/1 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2043, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1458/3558412713.py\", line 66, in search_face_cutouts\n",
      "    audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
      "  File \"/mnt/workspace/group_photo_detection/util.py\", line 183, in recognize_speech_from_audio\n",
      "    rec_result = speech_recognizer(audio_file, lang=\"zh\")\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/pipelines/audio/funasr_pipeline.py\", line 73, in __call__\n",
      "    output = self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/base/base_model.py\", line 35, in __call__\n",
      "    return self.postprocess(self.forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/modelscope/models/audio/funasr/model.py\", line 61, in forward\n",
      "    output = self.model.generate(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 301, in generate\n",
      "    return self.inference(input, input_len=input_len, **cfg)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/auto/auto_model.py\", line 343, in inference\n",
      "    res = model.inference(**batch, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/models/paraformer/model.py\", line 486, in inference\n",
      "    speech, speech_lengths = extract_fbank(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/funasr/utils/load_utils.py\", line 199, in extract_fbank\n",
      "    data_len.append(data_i.shape[0])\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.outputs import OutputKeys\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from util import *\n",
    "import cv2\n",
    "\n",
    "face_detector = pipeline(Tasks.face_detection, model='gaosheng/face_detect')\n",
    "# face_recognizer = pipeline(Tasks.face_recognition, model='damo/cv_ir101_facerecognition_cfglint')\n",
    "face_recognizer = pipeline(Tasks.face_recognition, model='iic/cv_ir101_facerecognition_cfglint')\n",
    "emotion_recognizer = pipeline(Tasks.facial_expression_recognition, 'damo/cv_vgg19_facial-expression-recognition_fer')\n",
    "portrait_matting = pipeline(Tasks.portrait_matting, model='damo/cv_unet_image-matting')\n",
    "speech_recognizer = pipeline(task=Tasks.auto_speech_recognition, model='iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch', device='cpu')\n",
    "face_bank = load_face_bank('face_bank/', face_recognizer)\n",
    "name_box_map = {}\n",
    "detected_image = None\n",
    "original_image = None\n",
    "\n",
    "\n",
    "def inference(img: Image, draw_detect_enabled, detect_threshold, sim_threshold) -> json:\n",
    "    global original_image\n",
    "    original_image = copy.deepcopy(img)\n",
    "\n",
    "    img = resize_img(img)\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "    global detected_image\n",
    "    detected_image = copy.deepcopy(img)\n",
    "\n",
    "    detection_result = face_detector(img)\n",
    "    boxes = np.array(detection_result[OutputKeys.BOXES])\n",
    "    scores = np.array(detection_result[OutputKeys.SCORES])\n",
    "    faces = []\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        score = scores[i]\n",
    "        if score < detect_threshold:\n",
    "            continue\n",
    "        box = boxes[i]\n",
    "        face_embedding = get_face_embedding(img, box, face_recognizer)\n",
    "        name, sim = get_name_sim(face_embedding, face_bank)\n",
    "        if name is None:\n",
    "            continue\n",
    "        if sim < sim_threshold:\n",
    "            faces.append({'box': box, 'name': '未知', 'sim': sim})\n",
    "        else:\n",
    "            faces.append({'box': box, 'name': name, 'sim': sim})\n",
    "            real_name = name[2:] # 去掉前2位学号\n",
    "            name_box_map[real_name] = box\n",
    "    rows = get_rows(faces)\n",
    "    row_names = get_row_names(faces, rows)\n",
    "    draw_name(img, row_names)\n",
    "    if draw_detect_enabled:\n",
    "        draw_faces(img, faces, emotion_recognizer)\n",
    "    return img, get_row_names_text(row_names)\n",
    "\n",
    "def search_face_cutouts(name_input, audio_input):\n",
    "\n",
    "    name = name_input\n",
    "    if not name:\n",
    "        audio_text = recognize_speech_from_audio(audio_input, speech_recognizer)\n",
    "        name_pinyin_bank = load_name_pinyin_bank('face_bank/')\n",
    "        found_name, sim = find_name_by_audio_text(audio_text, name_pinyin_bank)\n",
    "        if sim >= 0.3:\n",
    "            name = found_name\n",
    "\n",
    "    if not name:\n",
    "        return \"404.jpg\"\n",
    "\n",
    "    if name not in name_box_map:\n",
    "        return \"404.jpg\"\n",
    "\n",
    "    # 适当扩大边框范围，保证覆盖人脸但是又不会显得边框过大\n",
    "    box = name_box_map[name]\n",
    "    box[0] = box[0] - 5\n",
    "    box[2] = box[2] + 2\n",
    "    box[1] = box[1] - 2\n",
    "    box[3] = box[3] + 2\n",
    "\n",
    "    global original_image\n",
    "    original_image = original_image.convert('RGB')\n",
    "    original_image_box = []\n",
    "    original_image_box.append(original_image.width*(box[0]/detected_image.width))\n",
    "    original_image_box.append(original_image.height*(box[1]/detected_image.height))\n",
    "    original_image_box.append(original_image.width*(box[2]/detected_image.width))\n",
    "    original_image_box.append(original_image.height*(box[3]/detected_image.height))\n",
    "\n",
    "    face_img = get_face_img(original_image, original_image_box)\n",
    "    result = portrait_matting(face_img)\n",
    "    face_cutouts = result[OutputKeys.OUTPUT_IMG]\n",
    "    # 要先写成本地图片才能保存颜色(原始的npy array会导致丢部分颜色信息)\n",
    "    cv2.imwrite('temp.png', face_cutouts)\n",
    "    # 抠图之后的图像太小，需要等比放大一些\n",
    "    # face_cutouts = Image.open('temp.png')\n",
    "    # original_width, original_height = face_cutouts.size\n",
    "    # new_width = original_width * 5\n",
    "    # new_height = original_height * 5\n",
    "    # resized_face_cutouts = face_cutouts.resize((new_width, new_height), Image.LANCZOS)\n",
    "    return 'temp.png'\n",
    "\n",
    "examples = ['example.jpg']\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        draw_detect_enabled = gr.Checkbox(label=\"是否画框\", value=True)\n",
    "        detect_threshold = gr.Slider(label=\"检测阈值\", minimum=0, maximum=1, value=0.3)\n",
    "        sim_threshold = gr.Slider(label=\"识别阈值\", minimum=0, maximum=1, value=0.3)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            img_input = gr.Image(type=\"pil\", height=350)\n",
    "            submit = gr.Button(\"提交待识别图片\")\n",
    "        with gr.Column():\n",
    "            img_output = gr.Image(type=\"pil\", label=\"识别结果\")\n",
    "            name_output = gr.Text(label=\"人名列表\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            name_input = gr.Text(label=\"文本输入人名\")\n",
    "            audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"语音输入人名\")\n",
    "            submit2 = gr.Button(\"根据人名搜索头像\")\n",
    "        with gr.Column():\n",
    "            face_cutouts = gr.Image(type=\"pil\", label=\"搜索结果\")\n",
    "    submit.click(\n",
    "        fn=inference,\n",
    "        inputs=[img_input, draw_detect_enabled, detect_threshold, sim_threshold],\n",
    "        outputs=[img_output, name_output])\n",
    "    submit2.click(\n",
    "        fn=search_face_cutouts,\n",
    "        inputs=[name_input, audio_input],\n",
    "        outputs=[face_cutouts])\n",
    "    gr.Examples(examples, inputs=[img_input])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
